{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the path to import from common\n",
    "sys.path.append('../..')\n",
    "from common.datasets import CubeDataset, AFAContextDataset, MNISTDataset, DiabetesDataset\n",
    "from common.registry import AFA_DATASET_REGISTRY\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "# Define a callback to track metrics\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Get the last logged train loss\n",
    "        if 'train_loss' in trainer.callback_metrics:\n",
    "            self.train_losses.append(trainer.callback_metrics['train_loss'].item())\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Get the last logged validation loss and accuracy\n",
    "        if 'val_loss' in trainer.callback_metrics:\n",
    "            self.val_losses.append(trainer.callback_metrics['val_loss'].item())\n",
    "        if 'val_acc' in trainer.callback_metrics:\n",
    "            self.val_accuracies.append(trainer.callback_metrics['val_acc'].item())\n",
    "\n",
    "# Define the neural network model\n",
    "class FCNModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, class_weights=torch.tensor([0.5,0.5]), learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        # Build the layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y.long(), weight=self.class_weights)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y.long(), weight=self.class_weights)\n",
    "        self.log('val_loss', val_loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('val_acc', acc)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        test_loss = F.cross_entropy(y_hat, y.long(), weight=self.class_weights)\n",
    "        self.log('test_loss', test_loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('test_acc', acc)\n",
    "        return test_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "# Function to load a dataset from the data folder\n",
    "def load_dataset_from_file(dataset_name, split_idx=0, data_dir=\"data\"):\n",
    "    if dataset_name not in AFA_DATASET_REGISTRY:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not found in registry. Available datasets: {list(AFA_DATASET_REGISTRY.keys())}\")\n",
    "    \n",
    "    dataset_class = AFA_DATASET_REGISTRY[dataset_name]\n",
    "    \n",
    "    # Construct paths to the saved dataset splits\n",
    "    train_path = os.path.join(data_dir, dataset_name, f\"train_split_{split_idx+1}.pt\")\n",
    "    val_path = os.path.join(data_dir, dataset_name, f\"val_split_{split_idx+1}.pt\")\n",
    "    test_path = os.path.join(data_dir, dataset_name, f\"test_split_{split_idx+1}.pt\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not all(os.path.exists(path) for path in [train_path, val_path, test_path]):\n",
    "        raise FileNotFoundError(f\"Dataset splits not found. Please run generate_datasets.py first.\")\n",
    "    \n",
    "    # Load the datasets\n",
    "    train_dataset = dataset_class.load(train_path)\n",
    "    val_dataset = dataset_class.load(val_path)\n",
    "    test_dataset = dataset_class.load(test_path)\n",
    "    \n",
    "    # Get input and output dimensions from the training dataset\n",
    "    features, labels = train_dataset.get_all_data()\n",
    "    input_dim = features.shape[1]\n",
    "    output_dim = len(torch.unique(labels))\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name} (split {split_idx+1})\")\n",
    "    print(f\"Input dimension: {input_dim}\")\n",
    "    print(f\"Output dimension: {output_dim}\")\n",
    "    print(f\"Train size: {len(train_dataset)}\")\n",
    "    print(f\"Val size: {len(val_dataset)}\")\n",
    "    print(f\"Test size: {len(test_dataset)}\")\n",
    "    print(f\"Class distribution (train): {torch.bincount(labels.long(), minlength=output_dim)}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, input_dim, output_dim\n",
    "\n",
    "# Function to create data loaders\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size=64):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, max_epochs=50):\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Create metrics callback\n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stopping, checkpoint_callback, metrics_callback],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    return trainer, metrics_callback\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    trainer = pl.Trainer(accelerator='auto', devices=1)\n",
    "    results = trainer.test(model, test_loader)\n",
    "    return results\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, y = batch\n",
    "            y_hat = model(x)\n",
    "            preds = torch.argmax(y_hat, dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(metrics_callback):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics_callback.train_losses, label='Train Loss')\n",
    "    plt.plot(metrics_callback.val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics_callback.val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def run_dataset(dataset_name, split_idx, data_dir):\n",
    "    print(f\"Running {dataset_name} split {split_idx}\")\n",
    "    # Load dataset splits\n",
    "    train_dataset, val_dataset, test_dataset, input_dim, output_dim = load_dataset_from_file(\n",
    "        dataset_name, split_idx, data_dir\n",
    "    )\n",
    "\n",
    "    ## Normalize the data\n",
    "    ## Convert to numpy for sklearn preprocessing\n",
    "    #train_features = train_dataset.features.numpy()\n",
    "    #val_features = val_dataset.features.numpy()\n",
    "    #test_features = test_dataset.features.numpy()\n",
    "    \n",
    "    ## Create and fit the scaler on training data only\n",
    "    #scaler = StandardScaler()\n",
    "    ##scaler = MinMaxScaler()\n",
    "    #train_features_normalized = scaler.fit_transform(train_features)\n",
    "    \n",
    "    ## Transform validation and test data using the same scaler\n",
    "    #val_features_normalized = scaler.transform(val_features)\n",
    "    #test_features_normalized = scaler.transform(test_features)\n",
    "    \n",
    "    # Convert back to tensors\n",
    "   # train_dataset.features = torch.tensor(train_features_normalized, dtype=torch.float32)\n",
    "   # val_dataset.features = torch.tensor(val_features_normalized, dtype=torch.float32)\n",
    "   # test_dataset.features = torch.tensor(test_features_normalized, dtype=torch.float32)\n",
    "   \n",
    "    \n",
    "    print(f\"Data normalized. Feature statistics after normalization:\")\n",
    "    print(f\"  Train - Mean: {train_dataset.features.mean():.4f}, Std: {train_dataset.features.std():.4f}\")\n",
    "    print(f\"  Val - Mean: {val_dataset.features.mean():.4f}, Std: {val_dataset.features.std():.4f}\")\n",
    "    print(f\"  Test - Mean: {test_dataset.features.mean():.4f}, Std: {test_dataset.features.std():.4f}\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_dataset, val_dataset, test_dataset, batch_size=64\n",
    "    )\n",
    "\n",
    "    # Define model architecture\n",
    "    hidden_dims = [128, 64, 32]\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Calculate class weights based on class distribution\n",
    "    class_weights = torch.bincount(train_dataset.labels.long(), minlength=output_dim)/len(train_dataset)\n",
    "    # Invert the weights to give more importance to minority classes\n",
    "    class_weights = 1.0 / class_weights\n",
    "    # Normalize the weights\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "    # Create model\n",
    "    model = FCNModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=output_dim,\n",
    "        learning_rate=learning_rate,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer, metrics_callback = train_model(model, train_loader, val_loader, max_epochs=100)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions, labels = make_predictions(model, test_loader)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == labels).float().mean()\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if output_dim == 2:\n",
    "        f1score = f1_score(labels, predictions)\n",
    "        print(f\"Test F1 score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Github_Projects\\AFA-Benchmark\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Github_Projects\\AFA-Benchmark\\notebooks\\datasets\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 16.2 K | train\n",
      "---------------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running physionet split 0\n",
      "Dataset: physionet (split 1)\n",
      "Input dimension: 41\n",
      "Output dimension: 2\n",
      "Train size: 8400\n",
      "Val size: 1800\n",
      "Test size: 1800\n",
      "Class distribution (train): tensor([7207, 1193])\n",
      "Data normalized. Feature statistics after normalization:\n",
      "  Train - Mean: 65.4028, Std: 134.6499\n",
      "  Val - Mean: 65.1504, Std: 128.9722\n",
      "  Test - Mean: 65.1915, Std: 127.6785\n",
      "Class weights: tensor([0.1420, 0.8580])\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Github_Projects\\AFA-Benchmark\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\Github_Projects\\AFA-Benchmark\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 132/132 [00:00<00:00, 150.40it/s, v_num=51]\n",
      "Test accuracy: 0.7711\n",
      "Test F1 score: 0.4901\n"
     ]
    }
   ],
   "source": [
    "# Choose dataset\n",
    "dataset_name = \"physionet\"  # Options: \"cube\", \"AFAContext\", \"MNIST\", \"diabetes\", \"physionet\"\n",
    "split_idx = 0  # Which split to use (0-based index)\n",
    "data_dir = \"../../data\"  # Directory where the dataset splits are saved\n",
    "\n",
    "run_dataset(dataset_name, split_idx, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on all datasets\n",
    "#dataset_name = \"MNIST\"  # Options: \"cube\", \"AFAContext\", \"MNIST\", \"diabetes\"\n",
    "split_idx = 0  # Which split to use (0-based index)\n",
    "data_dir = \"../../data\"  # Directory where the dataset splits are saved\n",
    "\n",
    "for dataset_name in AFA_DATASET_REGISTRY.keys():\n",
    "    run_dataset(dataset_name, split_idx, data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
