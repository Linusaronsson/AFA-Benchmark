batch_size: 128
lr: 0.001
nepochs: 250
patience: 5

architecture:
  hidden_units: [128,128]
  dropout: 0.3
  activations: 'ReLU'
  flag_drop_out: True
  flag_only_output_layer: False

device: "cuda"
