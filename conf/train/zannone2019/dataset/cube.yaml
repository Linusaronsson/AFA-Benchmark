pretrained_model_artifact_name: "pretrain_zannone2019-MNIST_split_1:tmp"
n_agents: 128 # same as shim2018
hard_budget: 5 # recommended: 3, 5, 10
agent:
  # gamma set by higher config
  lmbda: 0.95

  clip_epsilon: 0.2
  entropy_bonus: True
  entropy_coef: 0.0001
  critic_coef: 1.0
  loss_critic_type: "smooth_l1"

  num_epochs: 1 # same as shim2018
  lr: 1e-3
  max_grad_norm: 1
  replay_buffer_batch_size: 128 # same as shim2018

  value_num_cells: [32, 32] # same complexity as shim2018
  value_dropout: 0.0
  policy_num_cells: [32, 32] # same complexity as shim2018
  policy_dropout: 0.0
n_batches: 15000 # up to 30k also works well, but not significant improvement
batch_size: 512 # same as shim2018
eval_every_n_batches: 500
eval_max_steps: 9001
n_eval_episodes: 100
n_generated_samples: 10000 # ODIN paper
generation_batch_size: 100
evaluate_final_performance: true
eval_only_n_samples: null
