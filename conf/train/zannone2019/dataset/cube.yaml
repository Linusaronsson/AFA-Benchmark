pretrained_model_artifact_name: "pretrain_zannone2019-cube_split_1:tmp"
evaluate_final_performance: true
eval_only_n_samples: null
n_agents: 128
hard_budget: 5 # recommended: 3, 5, 10
agent:
  gamma: 1
  lmbda: 0.95
  clip_epsilon: 0.2
  entropy_bonus: True
  entropy_coef: 0.0001
  critic_coef: 1.0
  loss_critic_type: "smooth_l1"
  lr: 0.001
  max_grad_norm: 1
  sub_batch_size: 128
  num_epochs: 4
  value_num_cells: [128, 128]
  value_dropout: 0.1
  policy_num_cells: [128, 128]
  policy_dropout: 0.1
n_batches: 15000 # up to 30k also works well, but not significant improvement
batch_size: 512
eval_every_n_batches: 500
eval_max_steps: 9001
n_eval_episodes: 100
n_generated_samples: 1000 # TODO: decide what this should be
generation_batch_size: 1000 # all cube samples fit easily on GPU
