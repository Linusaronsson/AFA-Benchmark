pretrained_model_artifact_name: "pretrain_zannone2019-MNIST_split_1:tmp"
n_agents: 2 # same as shim2018
hard_budget: 10 # recommended: 10, 20, 30
agent:
  # gamma set by higher config
  lmbda: 0.95

  clip_epsilon: 0.2
  entropy_bonus: True
  entropy_coef: 0.0001
  critic_coef: 1.0
  loss_critic_type: "smooth_l1"

  num_epochs: 1 # same as shim2018
  lr: 1e-3
  max_grad_norm: 1
  replay_buffer_batch_size: 32 # same as shim2018

  value_num_cells: [32, 32] # not specified by ODIN paper, same complexity as shim2018
  value_dropout: 0.0
  policy_num_cells: [32, 32] # not specified by ODIN paper, same complexity as shim2018
  policy_dropout: 0.0
# n_batches: 15000 # up to 30k also works well, but not significant improvement
n_batches: 200000 # hope we get some convergence after this
batch_size: 64 # same as shim2018
eval_every_n_batches: 500
eval_max_steps: 9001
n_eval_episodes: 100
# n_generated_samples: 10000 # ODIN paper
n_generated_samples: 0
generation_batch_size: 100
evaluate_final_performance: true
eval_only_n_samples: 100 # MNIST too large to evaluate every sample
