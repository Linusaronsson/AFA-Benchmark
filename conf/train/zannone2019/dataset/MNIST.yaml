pretrained_model_artifact_name: "pretrain_zannone2019-MNIST_split_1:tmp"
evaluate_final_performance: true
eval_only_n_samples: 100
n_agents: 2
hard_budget: 10 # recommended: 10, 50, 100
agent:
  gamma: 1
  lmbda: 0.95
  clip_epsilon: 0.2
  entropy_bonus: True
  entropy_coef: 0.0001
  critic_coef: 1.0
  loss_critic_type: "smooth_l1"
  lr: 0.001
  max_grad_norm: 1
  sub_batch_size: 128
  num_epochs: 4
  value_num_cells: [128, 128]
  value_dropout: 0.1
  policy_num_cells: [128, 128]
  policy_dropout: 0.1
n_batches: 15000 # up to 30k also works well, but not significant improvement
batch_size: 256
eval_every_n_batches: 500
eval_max_steps: 9001
n_eval_episodes: 100
# n_generated_samples: 42000 # Same as MNIST dataset size
n_generated_samples: 0
generation_batch_size: 100
