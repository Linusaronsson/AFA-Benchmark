dataset_artifact_name: "AFAContextRandomInsert_split_1:tmp"
batch_size: 100 # EDDI
epochs: 5000
limit_train_batches: null
limit_val_batches: null

lr: 1e-3
min_masking_probability: 0.0
max_masking_probability: 0.9
pointnet:
  type: "pointnetplus" # ODIN paper does not mention which type they use
  identity_size: 20 # EDDI (MNIST), not specified by zannone2019
  max_embedding_norm: 1.0
  feature_map_encoder_num_cells: [500] # EDDI (MNIST)
  feature_map_encoder_activation_class: "ReLU"
  feature_map_encoder_dropout: 0.0
  # output_size: 40 # always twice as large as latent size according to EDDI paper
  output_size: 20 # zannone2019
encoder:
  # num_cells: [500, 500, 200] # EDDI (MNIST)
  num_cells: [20, 500, 200] # zannone2019
  activation_class: "ReLU"
  dropout: 0.0
partial_vae:
  latent_size: 20 # EDDI (MNIST)
  # decoder_num_cells: [200, 500, 500] # EDDI (MNIST)
  decoder_num_cells: [500, 200] # zannone2019 (cube)
  decoder_activation_class: "ReLU"
  decoder_dropout: 0.0
classifier:
  num_cells: [128, 128] # zannone2019 paper does not say what this should be, so we use the same num_cells as the "common" classifier
  activation_class: "ReLU"
  dropout: 0.0
start_kl_scaling_factor: 0.1
end_kl_scaling_factor: 0.1
n_annealing_epochs: 1
classifier_loss_scaling_factor: 1
