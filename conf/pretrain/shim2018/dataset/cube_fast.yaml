dataset_artifact_name: "cube_split_1:tmp"
batch_size: 128 # shim2018 paper
epochs: 1 # fast
limit_train_batches: 1 # fast
limit_val_batches: 1 # fast

lr: 1e-3
min_masking_probability: 0.0
max_masking_probability: 0.9
encoder:
  output_size: 16 # shim2018 paper
  reading_block_cells: [32, 32] # shim2018 paper
  writing_block_cells: [32, 32] # not specified by shim2018 paper, assume it is the same as reading_block_cells
  memory_size: 16 # shim2018 paper
  processing_steps: 5 # original set encoding paper
  dropout: 0.0
classifier:
  num_cells: [128, 128] # same for all methods
