dataset_artifact_name: "MNIST_split_1:tmp"
batch_size: 128 # shim2018 paper
epochs: 200
limit_train_batches: null
limit_val_batches: null

lr: 1e-3
min_masking_probability: 0.75
max_masking_probability: 0.99
pq_module:
  n_hiddens: [128,128,128] # kachuee2019 does not list this for MNIST, so we have the same number of layers but larger than their architecture for the diabetes dataset.
  p_dropout: 0.5 # kachuee2019 implementation
