from pathlib import Path
from jaxtyping import Float
import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning as pl


from typing import Self, final, override

from afa_rl.utils import mask_data
from common.config_classes import Kachuee2019PQModuleConfig
from common.custom_types import (
    AFAClassifier,
    AFAPredictFn,
    FeatureMask,
    Features,
    Label,
    MaskedFeatures,
)


@final
class Kachuee2019PQModule(nn.Module):
    """The architecture proposed in the paper "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams", slightly simplified from the implementation found at https://github.com/mkachuee/Opportunistic/blob/master/Demo_OL_DQN.ipynb"""

    def __init__(self, n_features: int, n_classes: int, cfg: Kachuee2019PQModuleConfig):
        super().__init__()

        self.n_features = n_features
        self.n_classes = n_classes
        self.cfg = cfg

        # Create network
        self.layers_p = nn.ModuleList()
        self.layers_q = nn.ModuleList()

        # initialize the P-Net
        size_last = self.n_features
        for n_h in self.cfg.n_hiddens + [self.n_classes]:
            self.layers_p.append(nn.Linear(size_last, n_h))
            size_last = n_h

        # initialize the Q-Net
        size_last = self.n_features
        # always share_pq
        self.n_hiddens_q = []
        for ind in range(len(self.cfg.n_hiddens)):
            if ind == 0:
                self.n_hiddens_q.append(self.cfg.n_hiddens[ind])
            else:
                n_h = (self.cfg.n_hiddens[ind - 1] * self.cfg.n_hiddens[ind]) // (
                    self.cfg.n_hiddens[ind - 1] + self.n_hiddens_q[-1]
                )
                self.n_hiddens_q.append(n_h)
        for n_h, n_h_q in zip(self.cfg.n_hiddens, self.n_hiddens_q):
            self.layers_q.append(nn.Linear(size_last, n_h_q))
            size_last = n_h + n_h_q
        # Output of Q-Net does not include the stop action, unlike the original implementation
        self.layers_q.append(nn.Linear(size_last, self.n_features))

    @override
    def forward(
        self, masked_features: MaskedFeatures
    ) -> tuple[torch.Tensor, torch.Tensor]:
        # P-Net forward path
        act_last = masked_features
        acts_p = []
        for f_layer in self.layers_p[:-1]:
            act_last = F.dropout(F.relu(f_layer(act_last)), p=self.cfg.p_dropout)
            acts_p.append(act_last)
        class_logits = self.layers_p[-1](act_last)

        # Q-Net forward path, gradients are not backpropagated to P-Net
        act_last = masked_features
        act_last = F.relu(self.layers_q[0](act_last))
        for f_layer, p_act in zip(self.layers_q[1:-1], acts_p[:-1]):
            p_act = p_act.detach()
            act_last = F.relu(f_layer(torch.cat([act_last, p_act], dim=1)))
        p_act = acts_p[-1].detach()
        qvalues = self.layers_q[-1](torch.cat([act_last, p_act], dim=1))

        return class_logits, qvalues

    def confidence(self, masked_features: MaskedFeatures, mcdrop_samples: int = 1):
        """
        calculate the confidence histogrram for each class given a sample.
        masked_features: input sample of shape (batch_size, n_features)
        mcdrop_samples: mc dropout samples to use
        """
        x_rep = masked_features.unsqueeze(1).expand(
            -1, mcdrop_samples, -1
        )  # (batch_size, mcdrop_samples, n_features)
        x_rep = x_rep.reshape(
            x_rep.shape[0] * mcdrop_samples, x_rep.shape[-1]
        )  # (batch_size*mcdrop_samples, n_features)
        class_logits, _qvalues = self.forward(
            x_rep
        )  # class_logits.shape = (batch_size*mcdrop_samples, n_classes)
        class_logits = class_logits.view(
            masked_features.shape[0], mcdrop_samples, -1
        )  # (batch_size, mcdrop_samples, n_classes)
        class_probabilities = class_logits.softmax(
            dim=-1
        )  # (batch_size, mcdrop_samples, n_classes)
        return class_probabilities.mean(dim=1)  # (batch_size, n_classes)

    def predict(self, x: torch.Tensor, mcdrop_samples: int = 1):
        """
        make class prediction for a sample.
        x: input sample
        mcdrop_samples: mc dropout samples to use
        """
        conf = self.confidence(x, mcdrop_samples)
        pred = torch.argmax(conf)
        return pred


@final
class LitKachuee2019PQModule(pl.LightningModule):
    def __init__(
        self,
        pq_module: Kachuee2019PQModule,
        class_probabilities: Float[torch.Tensor, "n_classes"],
        min_masking_probability: float = 0.0,
        max_masking_probability: float = 1.0,
        lr: float = 1e-3,
    ):
        super().__init__()
        self.save_hyperparameters(ignore=["pq_module"])
        self.pq_module = pq_module
        self.class_weight = 1 / class_probabilities
        self.class_weight = self.class_weight / torch.sum(self.class_weight)
        self.min_masking_probability = min_masking_probability
        self.max_masking_probability = max_masking_probability
        self.lr = lr

    @override
    def forward(
        self, masked_features: MaskedFeatures, feature_mask: FeatureMask
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Forward pass.

        Args:
            masked_features: currently observed features, with zeros for missing features
            feature_mask: indicator for missing features, 1 if feature is observed, 0 if missing

        Returns:
            embedding: the embedding of the input features
            classifier_output: the output of the classifier

        """
        class_logits, qvalues = self.pq_module(masked_features)
        return class_logits, qvalues

    @override
    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int):
        features: Features = batch[0]
        label: Label = batch[1]

        masking_probability = self.min_masking_probability + torch.rand(1).item() * (
            self.max_masking_probability - self.min_masking_probability
        )
        self.log("masking_probability", masking_probability, sync_dist=True)

        masked_features, feature_mask, _ = mask_data(features, p=masking_probability)
        class_logits, _ = self(masked_features, feature_mask)
        loss = F.cross_entropy(
            class_logits, label, weight=self.class_weight.to(class_logits.device)
        )
        self.log("train_loss", loss)
        return loss

    def _get_loss_and_acc(
        self,
        masked_features: MaskedFeatures,
        feature_mask: FeatureMask,
        y: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        class_logits, _ = self(masked_features, feature_mask)
        loss = F.cross_entropy(
            class_logits, y, weight=self.class_weight.to(class_logits.device)
        )
        predicted_class = torch.argmax(class_logits, dim=1)
        true_class = torch.argmax(y, dim=1)
        acc = (predicted_class == true_class).float().mean()
        return loss, acc

    @override
    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int):
        feature_values, y = batch

        # Mask features with minimum probability -> see many features (observations)
        feature_mask_many_observations = (
            torch.rand(feature_values.shape, device=feature_values.device)
            > self.min_masking_probability
        )
        feature_values_many_observations = feature_values.clone()
        feature_values_many_observations[feature_mask_many_observations == 0] = 0
        loss_many_observations, acc_many_observations = self._get_loss_and_acc(
            feature_values_many_observations, feature_mask_many_observations, y
        )
        self.log("val_loss_many_observations", loss_many_observations)
        self.log("val_acc_many_observations", acc_many_observations)

        # Mask features with maximum probability -> see few features (observations)
        feature_mask_few_observations = (
            torch.rand(feature_values.shape, device=feature_values.device)
            > self.max_masking_probability
        )
        feature_values_few_observations = feature_values.clone()
        feature_values_few_observations[feature_mask_few_observations == 0] = 0
        loss_few_observations, acc_few_observations = self._get_loss_and_acc(
            feature_values_few_observations, feature_mask_few_observations, y
        )
        self.log("val_loss_few_observations", loss_few_observations)
        self.log("val_acc_few_observations", acc_few_observations)

    @override
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)


@final
class Kachuee2019AFAPredictFn(AFAPredictFn):
    """A wrapper for Kachuee2019PQModule to make it compatible with the AFAPredictFn interface."""

    def __init__(self, pq_module: Kachuee2019PQModule):
        super().__init__()
        self.pq_module = pq_module

    @override
    def __call__(
        self,
        masked_features: MaskedFeatures,
        feature_mask: FeatureMask,
        features: Features | None,
        label: Label | None,
    ) -> Label:
        class_logits, _qvalues = self.pq_module.forward(masked_features)
        return class_logits.softmax(dim=-1)


@final
class Kachuee2019AFAClassifier(AFAClassifier):
    """A wrapper for Kachuee2019PQModule to make it compatible with the AFAClassifier interface."""

    def __init__(
        self,
        pq_module: Kachuee2019PQModule,
        device: torch.device,
    ):
        super().__init__()
        self._device = device
        self.pq_module = pq_module.to(self._device)

    @override
    def __call__(
        self,
        masked_features: MaskedFeatures,
        feature_mask: FeatureMask,
        features: Features | None,
        label: Label | None,
    ) -> Label:
        original_device = masked_features.device

        masked_features = masked_features.to(self._device)
        feature_mask = feature_mask.to(self._device)

        class_logits, _qvalues = self.pq_module(masked_features)
        return class_logits.softmax(dim=-1).to(original_device)

    @override
    def save(self, path: Path) -> None:
        torch.save(self.pq_module.cpu(), path)

    @classmethod
    @override
    def load(cls, path: Path, device: torch.device) -> Self:
        pq_module = torch.load(path, weights_only=False, map_location=device)
        return cls(pq_module, device)

    @override
    def to(self, device: torch.device) -> Self:
        self.pq_module = self.pq_module.to(device)
        return self

    @property
    @override
    def device(self) -> torch.device:
        return self._device
